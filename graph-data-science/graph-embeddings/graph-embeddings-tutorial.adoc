= Tutorial: Applied Graph Embeddings
:section: Graph Embeddings
:section-link: graph-data-science
:section-level: 1
:slug: applied-graph-embeddings
:level: Intermediate
:sectanchors:
:toc:
:toc-title: Contents
:toclevels: 1
:author: Mark Needham
:category: graph-data-science
:tags: graph-data-science, graph-algorithms, graph-embeddings, machine-learning
:gds-version: 1.3-preview

++++
<script src="https://cdn.jsdelivr.net/npm/vega@5"></script>
<script src="https://cdn.jsdelivr.net/npm/vega-lite@4"></script>
<!-- Import vega-embed -->
<script src="https://cdn.jsdelivr.net/npm/vega-embed@6"></script>
++++

.Goals
[abstract]
In this guide, we will learn how to apply graph embeddings to an example dataset in Neo4j.

.Prerequisites
[abstract]
Please have link:/download[Neo4j^] (version 4.0 or later) and link:/download-center/#algorithms[Graph Data Science Library^] (version 1.3 or later) downloaded and installed to use graph embeddings.
You will also need to have Python installed to follow the second half of this guide.

[role=expertise]
{level}

[#graph-embeddings]
Graph embeddings were introduced in version 1.3 of the link:/graph-data-science-library/[Graph Data Science Library^] (GDSL).
They can be used to create a fixed size vector representation for nodes in a graph.
In this guide we'll learn how to use these algorithms to generate embeddings and how to interpret them using visualization techniques.

[NOTE]
====
For background reading about graph embeddings, see the link:/developer/graph-embeddings[Graph Embeddings] Developer Guide.
====

The code examples used in this guide can be found in the https://github.com/neo4j-examples/graph-embeddings[neo4j-examples/graph-embeddings^] GitHub repository.

[#eroads-dataset]
== European Roads dataset

We're going to use a dataset of European Roads compiled by Lasse Westh-Nielsen and https://lassewesth.blogspot.com/2018/07/the-international-e-road-network-and.html[described in more detail in his blog post^].
The dataset contains towns and the roads connecting them.
https://github.com/neo4j-examples/graph-embeddings#importing-dataset[Instructions for importing the data^] are in the project repository.

We can see the schema in the diagram below:

.European Roads Graph Schema
image::https://dist.neo4j.com/wp-content/uploads/20200708013834/eroads-schema.svg[]

This diagram was generated by running `CALL db.schema.visualization()` in the link:/developer/neo4j-browser/[Neo4j Browser] after importing the data.

We're now ready to run graph embeddings over our graph.

[#running-random-projection]
== Random Projection

The simplest (and fastest) of the link:/docs/graph-data-science/{gds-version}/algorithms/node-embeddings/[available algorithms^] is link:/docs/graph-data-science/{gds-version}/algorithms/alpha/fastrp/fastrp/[Random Projection,^] so we'll start with that one.

We're going to first run the streaming version of this procedure, which returns a stream of node ids and embeddings.
The algorithm has the following mandatory config:

`nodeProjection` :: the node labels to use for our projected graph
`relationshipProjection` :: the relationship types to use for our projected graph
`embeddingSize` :: the size of the vector/list of numbers to create for each node
`maxIterations` :: the number of iterations to run

We can run the algorithm with the following query:

[source, cypher]
----
CALL gds.alpha.randomProjection.stream({
   nodeProjection: "Place",
   relationshipProjection: {
     eroad: {
       type: "EROAD",
       orientation: "UNDIRECTED"
    }
   },
   embeddingSize: 10,
   maxIterations: 1
})
YIELD nodeId, embedding
RETURN gds.util.asNode(nodeId).name AS place, embedding
LIMIT 5;
----

In `relationshipProjection`, we specify `orientation: "UNDIRECTED"` so that the direction of the `EROAD` relationship type is ignored on the projected graph that the algorithm runs against.

If we run the query, it will return the following output:

.Results
[opts=header, cols="1,5"]
|===
| place      | embedding
| "Larne"    | [0.0, 0.0, 0.0, 0.0, 0.18257418274879456, -0.18257418274879456, 0.5477225184440613, -0.18257418274879456, -0.18257418274879456, 0.3651483654975891]
| "Belfast"  | [0.09128709137439728, -0.18257418274879456, 0.09128709137439728, 0.09128709137439728, 0.09128709137439728, -0.27386125922203064, 0.27386125922203064, 0.18257418274879456, 0.09128709137439728, -0.27386125922203064]
| "Dublin"   | [0.0, -0.13693062961101532, -0.13693062961101532, 0.0, 0.0, -0.13693062961101532, 0.27386125922203064, -0.13693062961101532, -0.13693062961101532, 0.13693062961101532]
| "Wexford"  | [0.0, -0.27386125922203064, -0.13693062961101532, 0.0, 0.27386125922203064, -0.13693062961101532, 0.0, 0.13693062961101532, 0.13693062961101532, -0.13693062961101532]
| "Rosslare" | [-0.27386125922203064, -0.27386125922203064, -0.41079187393188477, 0.0, 0.13693062961101532, -0.41079187393188477, 0.27386125922203064, -0.27386125922203064, -0.27386125922203064, 0.0]
|===

[NOTE]
====
This procedure is non deterministic, so we'll get different results each time that we run it.
====

Everything looks fine so far, we've been successful in returning embeddings for each node.

Further exploration will be easier if we store the embeddings in Neo4j, so we're going to do that using the write version of the procedure.
We can store the embeddings by running the following query:

[source, cypher]
----
CALL gds.alpha.randomProjection.write({
   nodeProjection: "Place",
   relationshipProjection: {
     eroad: {
       type: "EROAD",
       orientation: "UNDIRECTED"
    }
   },
   embeddingSize: 10,
   maxIterations: 1,
   writeProperty: "embeddingRandomProjection"
});
----

.Results
[opts=header]
|===
| nodeCount | nodePropertiesWritten | createMillis | computeMillis | writeMillis | configuration
| 894       | 894                   | 16           | 37            | 22          | {maxIterations: 1, writeConcurrency: 4, sparsity: 3, normalizeL2: FALSE, concurrency: 4, normalizationStrength: 0.0, writeProperty: "embeddingRandomProjection", iterationWeights: [], embeddingSize: 10, nodeLabels: ["*"], sudo: FALSE, relationshipTypes: ["*"]}
|===

We're now going to explore the graph embeddings using the Python programming language, the Neo4j Python driver, and some popular Data Science libraries.

[NOTE]
====
The code examples used in this section are available https://github.com/neo4j-examples/graph-embeddings/tree/main/notebooks[in Jupyter notebook form^] in the project repository.
====

The required libraries can be installed by running the following command:

[source,bash]
----
pip install neo4j sklearn altair
----

Let's create a file called `roads.py` and paste the following statements:

[source, python]
----
from neo4j import GraphDatabase
from sklearn.manifold import TSNE
import numpy as np
import altair as alt
import pandas as pd

driver = GraphDatabase.driver("bolt://localhost", auth=("neo4j", "neo"))
----

The first few lines import the required library and the last line creates a connection to the Neo4j database.
You'll need to change the Bolt URL and credentials to match that of your own database.

We're going to use the driver to execute a Cypher query that returns the embedding for each place.
We'll then convert the results in a Pandas data frame:

[source, python]
----
with driver.session(database="neo4j") as session:
    result = session.run("""
    MATCH (p:Place)
    RETURN p.name AS place, p.embeddingRandomProjection AS embedding, p.countryCode AS country
    """)
    X = pd.DataFrame([dict(record) for record in result])
----

Now we're ready to start analyzing the data.

At the moment our embeddings are of size 10, but we need them to be of size 2 so that we can visualize them in 2 dimensions.
The https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding[t-SNE algorithm^] is a dimensionality reduction technique that can be used to reduce high dimensionality objects to 2 or 3 dimensions so that they can be better visualized.
We're going to use it to create x and y coordinates for each embedding.

The following code snippet applies t-SNE to the embeddings and then creates a data frame containing each place, its country, as well as x and y coordinates.

[source, python]
----
X_embedded = TSNE(n_components=2, random_state=6).fit_transform(list(X.embedding))

places = X.place
df = pd.DataFrame(data = {
    "place": places,
    "country": X.country,
    "x": [value[0] for value in X_embedded],
    "y": [value[1] for value in X_embedded]
})
----

We can then run the following code to create a scatterplot of our embeddings:

[source, python]
----
alt.Chart(df).mark_circle(size=60).encode(
    x='x',
    y='y',
    tooltip=['place', 'country']
).properties(width=700, height=400)
----

++++
<div id="vis-randomProjection"></div>

<script type="text/javascript">
  var spec = "https://raw.githubusercontent.com/neo4j-examples/graph-embeddings/main/notebooks/charts/randomProjection.json";
  vegaEmbed('#vis-randomProjection', spec).then(function(result) {
    // Access the Vega view instance (https://vega.github.io/vega/docs/api/view/) as result.view
  }).catch(console.error);
</script>
++++

There don't seem to be any clusters of points in our visualization.
It's also hard to tell what each point represents without hovering over them individually.

We can color each point based on their `country` property with the following code:

[source, text]
----
alt.Chart(df).mark_circle(size=60).encode(
    x='x',
    y='y',
    color='country',
    tooltip=['place', 'country']
).properties(width=700, height=400, title="Random Projection Embeddings")
----

++++
<div id="vis-randomProjection-color"></div>

<script type="text/javascript">
  var spec = "https://raw.githubusercontent.com/neo4j-examples/graph-embeddings/main/notebooks/charts/randomProjection-color.json";
  vegaEmbed('#vis-randomProjection-color', spec).then(function(result) {
    // Access the Vega view instance (https://vega.github.io/vega/docs/api/view/) as result.view
  }).catch(console.error);
</script>
++++

[#node2vec]
== Node2Vec

[#resources]
== Resources

* link:/developer/graph-embeddings[Graph Embeddings Developer Guide]
* link:/docs/graph-data-science/1.3-preview/algorithms/node-embeddings/[Node Embeddings Reference Documentation^]
